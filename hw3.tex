\documentclass{article}  
\usepackage{amsmath}
\begin{document}

\title{CSE 512 HW 3}
\author{Cole Conte}
\date{7/6/2020}

\maketitle

\section{Mistake bound in consistent online learning}
Every time the algorithm makes an error, the hypotheses \(h \in H\) that made the error will be removed from \(V_{t+1}\). Since the realizability assumption holds at each iteration, there must be a hypothesis \(h^*\) that correctly labels every observation. After \(|H| - 1 \) errors, we'll be left with only one hypothesis: \(h^*\), which will correctly label all future observations. So we have an upper bound on the number of errors: \(|H| - 1 \).

The above bound can't be a strict inequality because we can generate a sequence of observations and a hypothesis class such that each hypothesis besides \(h^*\) is eliminated one at a time, so we need \(|H| - 1 \) errors before we're left only with  \(h^* \). Here's a trivial example. Start by defining:
\begin{equation}
H  \text{ contains } h_i \text{ for i = 0, 1, 2 where }
h_i =
 \begin{cases}
                                   0 & \text{if $x<i$} \\
                                   1 & \text{if $x \geq i$} \\
                                   
\end{cases}
\end{equation}

Where \(h^*=h_1\). Then, if we take \(x_1 = 0.5\), \(h_0\) will misclassify and be eliminated. Then if we take \(x_2 = 1.5\), \(h_2\) will misclassify and be eliminated. Therefore we need \(|H| - 1 = 3-1 = 2\)  errors before we're left only with  \(h^* \), thus we need a strict inequality.

\section{Stochastic online k-means algorithm}
\begin{equation}
\begin{split}
G(S,k) = \sum_{i=1}^{m} \sum_{j=1}^{k}\delta_{ij}((x_{i1}-\mu_{j1})^2 + (x_{i2}-\mu_{j2})^2...) \\
\frac{\partial G(S,k)}{\partial \mu_j} = \sum_{i=1}^{m} \sum_{j=1}^{k}2\delta_{ij}(\mu_{j1} - x_{i1} + \mu_{j2} - x_{i2}...) = \sum_{i=1}^{m} \sum_{j=1}^{k}2\delta_{ij}(\mu_j - x_i) \\ 
\mu_j^{(t)} = \mu_j^{(t-1)} - \eta_{t-1} \frac{\partial G(S,k)}{\partial \mu_j^{(t-1)}} =  \mu_j^{(t-1)} - \eta_{t-1} \sum_{i=1}^{m} \sum_{j=1}^{k}2\delta_{ij}(\mu_j^{(t-1)} - x_{t-1}) \\
= \mu_j^{(t-1)} + \eta_{t-1} \sum_{i=1}^{m} \sum_{j=1}^{k}\delta_{ij}(x_{t-1} - \mu_j^{(t-1)} ) \\
\mu_j^{(t)} =  \mu_j^{(t-1)} + \eta_{t-1} (x_{t-1} - \mu_j^{(t-1)} )
\end{split}
\end{equation}

\section{Gaussian mixture model with identical variance}
We set the derivative of the likelihood equation equal to zero with respect to mean, covariance, and mixing coefficents:
Mean:
\begin{equation}
\begin{split}
0 = -\sum_{i=1}^{m}r(y_k;x_i) \sigma^2 (x_i - \mu_k) = -\sigma^2 \sum_{i=1}^{m}r(y_k;x_i)  (x_i - \mu_k) \\
= \sum_{i=1}^{m}r(y_k;x_i)  (x_i - \mu_k) \\ 
\mu_k = \frac{1}{\sum_{i=1}^{m}r(y_k;x_i)}\sum_{i=1}^{m}r(y_k;x_i) x_i
\end{split}
\end{equation}
Variance:
\begin{equation}
\begin{split}
\sigma^2_k = \frac{1}{\sum_{i=1}^{m}r(y_k;x_i)}\sum_{i=1}^{m}r(y_k;x_i) (x_i-\mu_k)^2
\end{split}
\end{equation}
Mixing Coefficients:
\begin{equation}
\begin{split}
c_k = \sum_{i=1}^{m}\frac{r(y_k;x_i)}{m}
\end{split}
\end{equation}

\section{Biased estimation of Gaussian variance}

\begin{equation}
\begin{split}
E [ \hat { \sigma } ^ { 2 } ] = E [ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } ( x _ { i } - \overline { x } ) ^ { 2 } ]  \\
= \frac { 1 } { N } E [ \sum _ { i = 1 } ^ { N } ( x _ { i } ^ { 2 } - 2 x _ { i } \overline { x } + \overline { x } ^ { 2 } ) ]  \\ 
= \frac { 1 } { N } E [ \sum _ {i = 1} ^ { N } x _ { i } ^ { 2 } - \sum _ {i = 1}^ { N } 2 x _ { i} \overline { x } + \sum_ {i = 1} ^ { N } \overline { x } ^ { 2 } ] \\
= \frac { 1 } { N } E [ \sum _ {i = 1} ^ { N } x _ { i } ^ { 2 } - 2N \overline { x }^{2} + N \overline { x } ^ { 2 } ] \\
= \frac { 1 } { N } E [ \sum _ {i = 1} ^ { N } x _ { i } ^ { 2 } - N \overline { x } ^ { 2 } ]  = E [  x _ { i } ^ { 2 } ]- E[\overline { x } ^ { 2 } ]  \\
= \sigma _ { x } ^ { 2 } + E [ x _ { i } ] ^ { 2 } - \sigma _ { \overline { x } } ^ { 2 } - E [ \overline { x }  ] ^ { 2 } \\
= \sigma _ { x } ^ { 2 } - \sigma _ { \overline x } ^ { 2 }  = \sigma _ { x } ^ { 2 } - \frac{1}{N^2}Var(\sum _ { i = 1 } ^ { N } ( x _ { i } )) \\ 
= \sigma _ { x } ^ { 2 } - \frac{N}{N^2}\sigma _ { x } ^ { 2 } =  \frac{N-1}{N}\sigma _ { x } ^ { 2 } \neq \sigma _ { x } ^ { 2 }
\end{split}
\end{equation}

Therefore  \(\hat { \sigma } ^ { 2 }\) is a biased estimator of \({ \sigma } ^ { 2 }\).



\section{Regularized Maximum Likelihood}
\begin{equation}
\begin{split}
L = -\frac{1}{m} [ log(\theta) + log(1-\theta) + \sum_{i=1}^{m} logP_\theta (x_i) ] \\
L_{new} = -\frac{1}{m} [ log(\theta) + log(1-\theta) + \sum_{i=1}^{m+2} logP_\theta (x_i) ] \\
= -\frac{1}{m} [ log(\theta) + log(1-\theta) + log (P_\theta (x_{m+1})) + log (P_\theta (x_{m+2})) + \sum_{i=1}^{m} logP_\theta (x_i) ] \\
= - \frac{1}{m} [ log(\theta) + log(1-\theta) + log(\frac{1}{\theta}) + log(\frac{1}{1-\theta})  + \sum_{i=1}^{m} logP_\theta (x_i) ] \\
= \frac{1}{m}[\sum_{i=1}^{m} -logP_\theta (x_i) ] = L_{emp}
\end{split}
\end{equation}
Therefore \(\hat{\theta} = \frac{\sum_{i=1}^{m}x_i}{m} \)

\begin{equation}
\begin{split}
P[|\theta - \theta^*| \geq \frac{1}{m+2} + \frac{\epsilon}{2}] = P[|\hat{\theta} - E[\hat{\theta}] +E[\hat{\theta}]  + \theta^*| \geq \frac{1}{m+2} + \frac{\epsilon}{2}] \\
\leq P[|\hat{\theta} - E[\hat{\theta}|] \geq \frac{1}{m+2} + \frac{\epsilon}{2}] + P[|E[\hat{\theta}]  + \theta^*| \geq \frac{1}{m+2} + \frac{\epsilon}{2}] \\
= P[| \frac{1}{m}\sum_{i=1}^{m}Z_i- \hat{\theta} |] \geq \frac{1}{m+2} + \frac{\epsilon}{2}] + P[| \frac{1}{m}\sum_{i=1}^{m}Z_i- \theta^* | \geq \frac{1}{m+2} + \frac{\epsilon}{2}] \\
\leq 2(2exp(-2m(\frac{1}{m+2} + \frac{\epsilon}{2})^2)) = 4exp(-2m(\frac{1}{m+2} + \frac{\epsilon}{2})^2)
\end{split}
\end{equation}

\section{Bernoulli mixture}







\end{document}  
