# gaussian-mixture-model

Implemented a Gaussian Mixture Model to predict digits (0-9) from 64 features.

# Usage:
```
python gmm_learner.py --components 1 --train optdigits.train --test optdigits.test
```
Components: number of distributions to use. 

# About GMMs

Gaussian Mixture Models combine multiple weighted Gaussian (normal) distributions to make predictions.
We use Expectation Maximization (EM) to tune the parameters of each normal distribution. 

# About This Project

To implement a GMM for digit prediction, we had to train 10 different models: one for each digit.
In testing, we compute the likelihood that the digit is generated by each of the mixture models, and then choose the model that returned the highest likelihood.

# Prediction Error

Using this dataset's train/test split (and the initial means and covariances fixed by our random.seed()):


| K | Test Error |
| --- | ---------- |
| 1 | 0.043 |
| 3 | 0.052 |
| 4 | 0.063 |

Here it appears that one normal distribution was more effective than a combination in making predictions (although some individual digit errors were lower with K=3 and K=4).

# Hyperparameters to Tune

This implementation runs the expectation-maximization step a fixed number of times; this number could be tuned, or a check for convergence could be implemented.
Like the k-means algorithm (GMMs using EM are often called "soft" k-means), this optimization step can lead to local minima; a smarter initialization step for each mean and covariance could speed up time to convergence, similar to the way k-means++ speeds up computation time with a smarter choice of initial cluster centers.

